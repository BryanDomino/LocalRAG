{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53a74a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the dependencies\n",
    "from qdrant_client import models, QdrantClient\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.vectorstores.qdrant import Qdrant\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate\n",
    "from peft import PeftModel, PeftConfig\n",
    "import ctranslate2\n",
    "#\n",
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "#\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "386eed9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the document that you need to parse, please change the location to where the pdf resides\n",
    "\n",
    "# Load 1 PDF file\n",
    "# loader = PyPDFLoader(\"/mnt/data/smuckers_poc/RAG/2024-First-Quarter-Results.pdf\")\n",
    "# or load an entire folder\n",
    "loader = PyPDFDirectoryLoader(\"/mnt/data/Local-RAG-Custom/nissan\")\n",
    "data = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43d7454a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1975 chunks in the documents\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(data)} chunks in the documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd18966",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='detailed explanations of the vehicle fea-\\ntures and operation.\\nCHARGING THE LI-ION BATTERY\\nWARNING\\nThe EV (Electric Vehicle) system uses\\na high voltage current. Failure to\\nfollow the proper handling instruc-\\ntions may cause serious injury or\\ndeath. Be sure to read the “CH.\\nCharging” section and follow the\\nprocedures and guidelines de-scribed.\\nCHECKING LI-ION BATTERY CHAR-\\nGING STATUS\\nThe Li-ion battery charge status can be\\nchecked on the NissanConnect® Services\\napp (if so equipped).\\nIf the Li-ion battery is not sufficiently\\ncharged, you can start charging the Li-\\nion battery via the remote charge func-\\ntion. For additional information, see\\n“Charging related remote function” (P.CH-37).LIFE WITH AN EV (Electric Vehicle)\\n(scene guide)' metadata={'source': '/mnt/data/Local-RAG-Custom/nissan/2023-nissan-ariya-owner-manual.pdf', 'page': 27}\n"
     ]
    }
   ],
   "source": [
    "# Pick a sample page\n",
    "print(data[random.randint(0, len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3501b468",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1975 1975\n"
     ]
    }
   ],
   "source": [
    "# Split the data into pages\n",
    "metadatas = []\n",
    "texts = []\n",
    "for row in data:\n",
    "  metadatas.append(row.metadata)\n",
    "  texts.append(row.page_content)\n",
    "print(len(metadatas),len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9fb5920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the prompt template to use for the QA bot\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question enclosed within  3 backticks at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Please provide an answer which is factually correct and based on the information retrieved from the vector store.\n",
    "Please also mention any quotes supporting the answer if any present in the context supplied within two double quotes \"\" .\n",
    "\n",
    "{context}\n",
    "\n",
    "QUESTION:```{question}```\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\",\"question\"])\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ad0061b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the embedding model\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embedding_model_name = \"BAAI/bge-small-en\"\n",
    "os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/mnt/data/Local-RAG-Custom/model_cache/'\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-small-en\",\n",
    "                                      model_kwargs=model_kwargs,\n",
    "                                      encode_kwargs=encode_kwargs\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d82a201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment this code if you want to store the embeddings in Qdrant in-memory\n",
    "# doc_store = Qdrant.from_texts(texts,\n",
    "#                              metadatas=metadatas,\n",
    "#                              embedding=embeddings,\n",
    "#                              location=\":memory:\",\n",
    "#                              collection=f\"{embedding_model_name}_press_release\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d185e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Persist the embeddings to disk\n",
    "doc_store = Qdrant.from_texts(texts,\n",
    "                              metadatas=metadatas,\n",
    "                              embedding=embeddings,\n",
    "                              path=\"/mnt/artifacts/notebook/local_qdrant/\",\n",
    "                              prefer_grpc=True,\n",
    "                              collection=f\"{embedding_model_name}_press_release\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0d1ca96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.70s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/mnt/data/Local-RAG-Custom/model_cache/tokenizer_config.json',\n",
       " '/mnt/data/Local-RAG-Custom/model_cache/special_tokens_map.json',\n",
       " '/mnt/data/Local-RAG-Custom/model_cache/tokenizer.model',\n",
       " '/mnt/data/Local-RAG-Custom/model_cache/added_tokens.json',\n",
       " '/mnt/data/Local-RAG-Custom/model_cache/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model and the tokenizer\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=\"/mnt/data/Local-RAG-Custom/model_cache/\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5a39225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the QA chain\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n",
    "rag_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    \n",
    "qa_chain = RetrievalQA.from_chain_type(llm=rag_llm,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       chain_type_kwargs={\"prompt\": PROMPT},\n",
    "                                       retriever=doc_store.as_retriever(search_kwargs={\"k\": 5}),\n",
    "                                       return_source_documents=True\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edee703e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please provide your question here : what is the tyre pressure?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tyre pressure for a Nissan Ariya is 30 psi (200 kPa) for the front tires and 26 psi (180 kPa) for the rear tires, according to the Tire and Loading Information label on the vehicle.\n",
      "\n",
      "QUOTE:\n",
      "\"The tire pressurizes as tire temperature rises.\" (WBI0031X)\n",
      "\n",
      "Please let me know if you need any further assistance.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "user_question = input(\"Please provide your question here :\")\n",
    "result = qa_chain(user_question)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ad0da92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "638a0b93",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9de03dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  9.79it/s]\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Converted successfully\n"
     ]
    }
   ],
   "source": [
    "# full_model_path = 'subirmansukhani/llama-2-7b-miniguanaco'\n",
    "full_model_path = '/mnt/data/Local-RAG-Custom/model_cache/models--NousResearch--Llama-2-7b-chat-hf/snapshots/37892f30c23786c0d5367d80481fa0d9fba93cf8'\n",
    "# Copy the file over from this path to /mnt/data/ or wherever your Domino datasets is if you intend to use this model in an app\n",
    "tokenizer.save_pretrained(full_model_path)\n",
    "ct2_path ='/mnt/data/Local-RAG-Custom/llama2/llama2-ct/'\n",
    "quantization ='float16'\n",
    "\n",
    "os.system(f\"sudo ct2-transformers-converter --model {full_model_path} --copy_files tokenizer.model --output_dir {ct2_path} --quantization {quantization}  --low_cpu_mem_usage\")\n",
    "\n",
    "print(\" Model Converted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa2e90a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA failed with error out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# load the ctranslate model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mctranslate2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA failed with error out of memory"
     ]
    }
   ],
   "source": [
    "model_path = '/mnt/data/Local-RAG-Custom/llama2/llama2-ct'\n",
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "model_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# load the ctranslate model\n",
    "generator = ctranslate2.Generator(model_path, device=model_device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ae8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the QA chain\n",
    "pipe = pipeline(\"text-generation\", model=generator, tokenizer=tokenizer, max_new_tokens=200)\n",
    "rag_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    \n",
    "qa_chain = RetrievalQA.from_chain_type(llm=rag_llm,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       chain_type_kwargs={\"prompt\": PROMPT},\n",
    "                                       retriever=doc_store.as_retriever(search_kwargs={\"k\": 5}),\n",
    "                                       return_source_documents=True\n",
    "                                      )"
   ]
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
