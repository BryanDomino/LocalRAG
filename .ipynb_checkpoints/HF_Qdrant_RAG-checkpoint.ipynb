{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7933badd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import all the dependencies\n",
    "from qdrant_client import models, QdrantClient\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.vectorstores.qdrant import Qdrant\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate\n",
    "from peft import PeftModel, PeftConfig\n",
    "#\n",
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "#\n",
    "import os\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "635ba0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the document that you need to parse, please change the location to where the pdf resides\n",
    "\n",
    "# Load 1 PDF file\n",
    "# loader = PyPDFLoader(\"/mnt/data/smuckers_poc/RAG/2024-First-Quarter-Results.pdf\")\n",
    "# or load an entire folder\n",
    "loader = PyPDFDirectoryLoader(\"/mnt/data/Local-RAG-Custom/nissan\")\n",
    "data = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e25fbdec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1975 chunks in the document\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(data)} chunks in the document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df219859",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='JVS0741X\\nIllustration 5 – Entering from the side\\nEntering from the side\\nIllustration 5: The side indicator light\\nilluminates if a vehicle enters the detec-tion zone from either side.\\nWAF0555X\\nIllustration 6 – Entering from the side\\nIllustration 6: If the driver activates theturn signal while another vehicle is in thedetection zone, then the system chimes\\n(twice) and the side indicator light and\\nBSW indicator flash.\\nNOTE:\\n.The radar sensors may not detect a\\nvehicle which is traveling at about\\nthe same speed as your vehicle\\nwhen it enters the detection zone.\\n.If the driver activates the turn signal\\nbefore a vehicle enters the detectionzone, the side indicator light and\\nBSW indicator will flash but no chime\\nwill sound when the other vehicle isdetected.SYSTEM TEMPORARILY UNAVAIL-\\nABLE\\nWhenradarblockageisdetected,theBSW\\nsystem will be turned off automatically\\nand the “Unavailable Side Radar Obstruc-\\ntion” warning message will appear in thevehicle information display.' metadata={'source': '/mnt/data/Local-RAG-Custom/nissan/2023-nissan-ariya-owner-manual.pdf', 'page': 400}\n"
     ]
    }
   ],
   "source": [
    "# Pick a sample page\n",
    "print(data[random.randint(0, len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0827bd3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1975 1975\n"
     ]
    }
   ],
   "source": [
    "# Split the data into pages\n",
    "metadatas = []\n",
    "texts = []\n",
    "for row in data:\n",
    "  metadatas.append(row.metadata)\n",
    "  texts.append(row.page_content)\n",
    "print(len(metadatas),len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e1b236",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the prompt template to use for the QA bot\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question enclosed within  3 backticks at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Please provide an answer which is factually correct and based on the information retrieved from the vector store.\n",
    "Please also mention any quotes supporting the answer if any present in the context supplied within two double quotes \"\" .\n",
    "\n",
    "{context}\n",
    "\n",
    "QUESTION:```{question}```\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\",\"question\"])\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a528bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)421f3/.gitattributes: 100%|██████████| 1.52k/1.52k [00:00<00:00, 522kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 68.9kB/s]\n",
      "Downloading (…)93c10421f3/README.md: 100%|██████████| 90.2k/90.2k [00:00<00:00, 60.3MB/s]\n",
      "Downloading (…)c10421f3/config.json: 100%|██████████| 684/684 [00:00<00:00, 740kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 124/124 [00:00<00:00, 136kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 133M/133M [00:03<00:00, 41.5MB/s] \n",
      "Downloading pytorch_model.bin: 100%|██████████| 134M/134M [00:00<00:00, 350MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 52.0/52.0 [00:00<00:00, 17.7kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 120kB/s]\n",
      "Downloading (…)421f3/tokenizer.json: 100%|██████████| 711k/711k [00:00<00:00, 37.0MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 366/366 [00:00<00:00, 355kB/s]\n",
      "Downloading (…)93c10421f3/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 66.4MB/s]\n",
      "Downloading (…)10421f3/modules.json: 100%|██████████| 349/349 [00:00<00:00, 338kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding model\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embedding_model_name = \"BAAI/bge-small-en\"\n",
    "os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/mnt/data/Local-RAG-Custom/model_cache/'\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-small-en\",\n",
    "                                      model_kwargs=model_kwargs,\n",
    "                                      encode_kwargs=encode_kwargs\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b7c85aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment this code if you want to store the embeddings in Qdrant in-memory\n",
    "# doc_store = Qdrant.from_texts(texts,\n",
    "#                              metadatas=metadatas,\n",
    "#                              embedding=embeddings,\n",
    "#                              location=\":memory:\",\n",
    "#                              collection=f\"{embedding_model_name}_press_release\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77f5384f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Persist the embeddings to disk\n",
    "doc_store = Qdrant.from_texts(texts,\n",
    "                              metadatas=metadatas,\n",
    "                              embedding=embeddings,\n",
    "                              path=\"/mnt/artifacts/local_qdrant/\",\n",
    "                              prefer_grpc=True,\n",
    "                              collection=f\"{embedding_model_name}_press_release\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09d3838b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:47<00:00, 23.72s/it]\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model and the tokenizer\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=\"/mnt/data/Local-RAG-Custom/model_cache/\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5b56bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the QA chain\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n",
    "rag_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    \n",
    "qa_chain = RetrievalQA.from_chain_type(llm=rag_llm,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       chain_type_kwargs={\"prompt\": PROMPT},\n",
    "                                       retriever=doc_store.as_retriever(search_kwargs={\"k\": 5}),\n",
    "                                       return_source_documents=True\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d18c3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please provide your question here : what is the weight limit?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weight limit for the vehicle is 7,250 lb (3,289 kg) GVWR.\n",
      "\n",
      "BACKUP:\n",
      "The weight limit for the vehicle is 7,250 lb (3,289 kg) GVWR, which is the maximum weight that the vehicle is designed to carry. This includes the weight of the vehicle itself, as well as the weight of any cargo or passengers that are loaded onto the vehicle. It is important to not exceed this weight limit, as doing so can cause damage to the vehicle or result in a loss of control. If you are unsure about the weight limit for your vehicle, you can find this information in the owner's manual or on the certification label on the vehicle.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "user_question = input(\"Please provide your question here :\")\n",
    "result = qa_chain(user_question)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf90d82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
