{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3e80a5",
   "metadata": {},
   "source": [
    "# Document Search using Retrieval Augmented Generation (RAG)\n",
    "\n",
    "In this project we demonstrate the use of a pre-trained Large Language Model (LLM) in Domino and the process of augmenting this model using Retrieval Augmented Generation (RAG) with documents to tailor to our use case. We will use the Meta's open source [Llama2 model](https://ai.meta.com/llama/) and [Qdrant vector database](https://qdrant.tech/) to enable us to run the entire chain on Domino.\n",
    "\n",
    "In this notebook we will:\n",
    "1. Fetch and Process the Documents\n",
    "2. Initialise the Vector Store\n",
    "3. Fetch and Initialise the Llama2 Model\n",
    "4. Create the QA chain and test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e522dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import all the dependencies\n",
    "from qdrant_client import models, QdrantClient\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.vectorstores.qdrant import Qdrant\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate\n",
    "from peft import PeftModel, PeftConfig\n",
    "#\n",
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "#\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b3c446",
   "metadata": {},
   "source": [
    "## Get Our Documents And Embeddings\n",
    "In this example we will read from a document in our repository in the sample_data folder.\n",
    "\n",
    "**Note: You will need to customise this section to your specific use case**\n",
    "\n",
    "Domino has many ways to access data. Please see our [documentation to find the method that suits your use case](https://docs.dominodatalab.com/en/latest/user_guide/16d9c1/work-with-data/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189ef9ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the document that you need to parse, please change the location to where the pdf resides\n",
    "\n",
    "# Load 1 PDF file\n",
    "# loader = PyPDFLoader(\"/mnt/code/sample_data/DominoDocumentation.pdf\")\n",
    "# or load an entire folder\n",
    "loader = PyPDFDirectoryLoader(\"/mnt/data/\" + os.environ['DOMINO_PROJECT_NAME'] + \"/\" + os.environ['CUSTOMER_NAME'])\n",
    "data = loader.load_and_split(RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ad7f77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1975 chunks in the documents\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(data)} chunks in the documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f473d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Change Assist” (P.5-113) for conditions)\\n.No vehicle is detected in the intended\\nlane\\n.Lane markers are detected\\n.Vehicle speed is above approximately\\n37 MPH (60 km/h)\\n.Driver’s hands must be detected onthe steering wheel\\n.Passing Assist is enabled\\nThe vehicle will suggest to pass when theslower lead vehicle is detected traveling\\nat the following speeds:\\nSetting Lead vehicle speed\\nSport 3 MPH + (5 km/h) slower\\nStandard 6 MPH + (10 km/h) slower\\nComfort 9 MPH + (15 km/h) slower\\nNOTE:\\nThe steering operation by the driver is\\nalways prioritized.\\nHow to enable/disable Passing Assist:\\nVehicle information displaySettings ?Driver Assistance ?Lane\\nChange Assist ?Passing Assist ?ON/\\nOFF\\nHow to change Passing Assist mode:\\nSettings ?Driver Assistance ?Lane\\nChange Assist ?Passing Setting ?\\nSport/Standard/ComfortSystem operation\\n1. When a slower vehicle is detected\\nahead, the message “Slow Vehicle\\nAhead\\n Change Lanes Left” ap-\\npears.\\n2. Ensure that it is safe to move into the' metadata={'source': '/mnt/data/Local-RAG-Customised/nissan/2023-nissan-ariya-owner-manual.pdf', 'page': 456}\n"
     ]
    }
   ],
   "source": [
    "# Pick a sample page\n",
    "print(data[random.randint(0, len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e94acf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1975 1975\n"
     ]
    }
   ],
   "source": [
    "# Split the data into pages\n",
    "metadatas = []\n",
    "texts = []\n",
    "for row in data:\n",
    "  metadatas.append(row.metadata)\n",
    "  texts.append(row.page_content)\n",
    "print(len(metadatas),len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfc8297f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the embedding model and cache it in our artifacts directory\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embedding_model_name = \"BAAI/bge-small-en\"\n",
    "os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/mnt/artifacts/model_cache/'\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-small-en\",\n",
    "                                      model_kwargs=model_kwargs,\n",
    "                                      encode_kwargs=encode_kwargs\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65fb38b",
   "metadata": {},
   "source": [
    "## Initialise The Vector Database\n",
    "Now we can create the collection in the Qdrant Vector Database.\n",
    "\n",
    "**Note: This step takes several minutes!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "859ef029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the embeddings to disk in our artifacts directory\n",
    "doc_store = Qdrant.from_texts(texts,\n",
    "                              metadatas=metadatas,\n",
    "                              embedding=embeddings,\n",
    "                              path=\"/mnt/artifacts/local_qdrant/\",\n",
    "                              prefer_grpc=True,\n",
    "                              collection_name=\"nissan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbccd7e",
   "metadata": {},
   "source": [
    "## Initialise The Model\n",
    "\n",
    "Now that we have the Vector Store and the Embedding Model we need to get the Foundation Model that we will be using.\n",
    "In this case we are leveraging the open source Llama2 model Llama-2-7b-chat-hf. In contrast to third party services like OpenAI this open source model allows you to download the model into your cloud and run it entirely in your enterprises ecosystem meaning you have tighter controls over security and governance.\n",
    "\n",
    "We will:\n",
    "1. Set up the prompt for this use case\n",
    "2. Configure bitsandbytes for the quantisation we need\n",
    "3. Download, configure and save the Llama2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2bb686",
   "metadata": {},
   "source": [
    "### 1. Set up the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00d77af6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the prompt template to use for the QA bot\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question enclosed within  3 backticks at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Please provide an answer which is factually correct and based on the information retrieved from the vector store.\n",
    "Please also mention any quotes supporting the answer if any present in the context supplied within two double quotes \"\" .\n",
    "\n",
    "{context}\n",
    "\n",
    "QUESTION:```{question}```\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\",\"question\"])\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b418d6cc",
   "metadata": {},
   "source": [
    "### 2. Configure bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "955ea963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure bitsandbytes\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b7b27a",
   "metadata": {},
   "source": [
    "### 3. Download and configure the model\n",
    "\n",
    "**Note: This step can take several minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eef9873a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 10.14it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:38<00:00, 49.31s/it]\n",
      "tokenizer_config.json: 100%|██████████| 746/746 [00:00<00:00, 434kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 5.16MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 9.02MB/s]\n",
      "added_tokens.json: 100%|██████████| 21.0/21.0 [00:00<00:00, 12.7kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 435/435 [00:00<00:00, 276kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=\"/mnt/artifacts/model_cache/\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab77b2f",
   "metadata": {},
   "source": [
    "## Putting it all together!\n",
    "\n",
    "Now we have our Vector Database with our documents in it and our configured model we can create our RAG QA chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05d371d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the QA chain\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n",
    "rag_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    \n",
    "qa_chain = RetrievalQA.from_chain_type(llm=rag_llm,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       chain_type_kwargs={\"prompt\": PROMPT},\n",
    "                                       retriever=doc_store.as_retriever(search_kwargs={\"k\": 5}),\n",
    "                                       return_source_documents=True\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7448d03f",
   "metadata": {},
   "source": [
    "Now we can test our model!\n",
    "\n",
    "Run the following cell and ask a question based on the documents you have added to the vector store. You may want to play with the max_new_tokens parameter in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c08656c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please provide your question here : how do I change the oil?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The owner's manual of the vehicle provides detailed instructions on how to change the oil. It is recommended to consult the manual for the specific steps and precautions to take when changing the oil in your vehicle.\n",
      "\n",
      "BACKGROUND:\n",
      "Changing the oil in a vehicle is an essential maintenance task that helps to keep the engine running smoothly and prolong its lifespan. It involves draining the old oil from the engine and replacing it with new oil. The frequency of oil changes varies depending on the vehicle make and model, as well as the driving conditions. It is typically recommended to change the oil every 5,000 to 7,500 miles, but it may be more frequent for some vehicles.\n",
      "\n",
      "RELATED QUOTES:\n",
      "\n",
      "* \"Changing the oil in your vehicle is an essential maintenance task that helps to keep the engine running smoothly and prolong its lifespan.\"\n",
      "* \"It\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "user_question = input(\"Please provide your question here :\")\n",
    "result = qa_chain(user_question)\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f72d13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
